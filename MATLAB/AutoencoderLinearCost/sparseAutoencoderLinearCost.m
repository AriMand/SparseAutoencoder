function [cost,grad] = sparseAutoencoderCost(W, visibleSize, hiddenSize, ...
    lambda, sparsityParam, beta, data)

% visibleSize: количество входных узлов (probably 64)
% hiddenSize: количество скрытых узнов (probably 25)
% lambda: коэффициент ослабления весов
% sparsityParam: Желаемый уровень активации нейронов скрытого слоя (Ро).
% beta: Коэффициент (вес) слагаемого отвечающего за разреженность.
% data: Наша матрица 64x10000 содержащая обучающую выборку.
% таким образом, data(:,i) это i-th обучающая пара (вход и выход, в данном случае одно и то-же).

% Входной параметр W это вектор (т.к. minFunc ожидает, что параметр является вектором).
% Сначала мы разобъем W на куски (W1, W2, b1, b2), чтобы все было как в лекции.

% Веса, соединяющие входной вектор и скрытый слой
W1 = reshape(W(1:hiddenSize*visibleSize), hiddenSize, visibleSize);
% Веса, соединяющие скрытый слой и выход
W2 = reshape(W(hiddenSize*visibleSize+1:2*hiddenSize*visibleSize), visibleSize, hiddenSize);
% Смещения нейронов скрытого слоя
b1 = W(2*hiddenSize*visibleSize+1:2*hiddenSize*visibleSize+hiddenSize);
% Смещения нейронов выходного слоя
b2 = W(2*hiddenSize*visibleSize+hiddenSize+1:end);

% ---------- ВАШ КОД ЗДЕСЬ --------------------------------------
%  Инструкции: Вычислите функцию потерь/функцию оптимизации J_sparse(W,b) для разреженного автонкодера,
%              и соотретствующие градиенты W1grad, W2grad, b1grad, b2grad.
%
% W1grad, W2grad, b1grad и b2grad вычисляются методом обратного распространения ошибки.
% Заметте, что W1grad должна иметь те же размеры что и W1,
% b1grad должна иметь те же размеры что и b1, и т.д.
% W1grad это частная производная J_sparse(W,b) по W1.
% Т.е., W1grad(i,j) это частная производная J_sparse(W,b)
% по параметру W1(i,j).  Таким образом, W1grad должна быть равна
% [(1/m) Delta W1 + lambda W1] в последнем блоке псевдокода секция 2.2
% лекций (и аналогично для W2grad, b1grad, b2grad).
%
% Другими словами, если мы используем пакетный метод градиентного спуска,
% на каждом шаге W1 будет уточняться по формуле: W1 := W1 - alpha * W1grad,
% аналогично для W2, b1, b2.

numPatches=size(data,2);

%----------------------------
% прямой проход (расчет выхода сети)
%----------------------------
z2=W1*data;
z2=bsxfun(@plus,z2,b1);
a2=sigmoid(z2);

z3=W2*a2;
z3=bsxfun(@plus,z3,b2);
a3=z3;
%a3=sigmoid(z3);

J=0.5*sum(  (a3(:)-data(:)).^2  );
avgActivations=sum(a2,2)./numPatches;
%----------------------------
% Вычисления, связанные с условием разреженности
% то есть чтобы в скрытом слое на поданный сигнал
% активировалось лишь небольшое количество нейронов
%----------------------------
% Добавляется к дельте скрытого слоя при обратном проходе
sparsity_grad=beta.*(-sparsityParam./avgActivations+((1-sparsityParam)./(1-avgActivations)));
% Слагаемые дивергенции Куллбэка-Лейблера
KL1=sparsityParam*log(sparsityParam./avgActivations);
KL2=(1-sparsityParam)*log((1-sparsityParam)./(1-avgActivations));
% дивергенция Куллбэка-Лейблера (сумма элементов по всей выборке данных)
KL_divergence=sum(KL1+KL2);
% Функция потерь (минимизируемый функционал)
cost=(1/numPatches)*J+lambda*0.5*(sum(sum(W1.^2))+sum(sum(W2.^2)))+beta*KL_divergence;
%----------------------------
% обратное распространение ошибки
%----------------------------
%ошибка выходного слоя
delta_3=(a3-data);%.*a3.*(1-a3);
%ошибка скрытого слоя
err2=bsxfun(@plus,W2'*delta_3,sparsity_grad);
delta_2=err2.*a2.*(1-a2);
%----------------------------
% Градиенты смещений
%----------------------------   
b1grad=sum(delta_2,2)/numPatches;
b2grad=sum(delta_3,2)/numPatches;
%----------------------------
% Градиенты весов
%----------------------------
W1grad=(delta_2*data')/numPatches+(lambda).*W1;
W2grad=(delta_3*a2')/numPatches+(lambda).*W2;

%----------------------------
% Соберем вычисленные значения
% градиентов в вектор-столбец
% (подходящий для minFunc).
%----------------------------
grad = [W1grad(:) ; W2grad(:) ; b1grad(:) ; b2grad(:)];

end

%-------------------------------------------------------------------
% Функция вычисления сигмоида
%-------------------------------------------------------------------
function sigm = sigmoid(x)
sigm = 1 ./ (1 + exp(-x));
end